# 21. Convolutional Neural Networks

# 21.2 Introduction

Convolution is a well-established mathematical technique that was 
around long before computers, and has been applied to problems in 
many different fields. For example, in audio processing we can apply 
convolution to an existing recording to make it sound like it was 
recorded in a small night club, or a giant concert hall, or even outdoors 
[Hass13]. If we want to send music over the airwaves using AM or FM, 
then convolution shows us how to build our transmitters and receivers 
[Oppenheim96]. 

Although convolution can be used to process many different kinds of 
data, in this chapter we’ll focus on image processing. To simplify 
our discussion, we’ll talk exclusively about 2D images. In machine 
learning terms, each image is a sample. Each pixel in a grayscale 
image is a single feature. If the image is in color, then there are three 
features per sample (one each for the red, green, and blue the values at 
each pixel).

Even if the input to a convolution layer is an image, the output will 
be a 3D tensor, or block. Unlike a grayscale image that represents its 
pixel data with 1 channel, or a color image that is made from 3 channels (one each for red, green, and blue), the tensor coming out of a 
convolution layer can have any number of channels. 

If we really want to visualize this tensor, we can peel away the top layer 
and draw it as a grayscale image, then do the same with the second 
layer, and so on. With this process in mind, some authors use the term 
“image” to refer the tensor that comes out of a convolution layer, but 
keep in mind that this terminology is a stretch. 

Since convolution layers are often arranged in series, with the output 
of one serving as the input to the next, the inputs and outputs of these 
layers can be tensors of any size.

We’ll see that it’s common to use multiple convolution layers in one 
network. A network where the convolution layers play a central role can 
be called a convolutional neural network, or convnet, or more 
commonly, a CNN. Sometimes people also say “CNN network” (an 
example of “redundant acronym syndrome syndrome” [Memmott15]). 
Before digging into convolution, we can save a world of confusion with 
a short detour into terminology.

# 21.2.1 The Two Meanings of “Depth”

There’s some unfortunate duplication of language that can get confusing if we don’t know it’s coming. The issue is the word depth, which 
carries two meanings. 

Every image has a size, given by its width and height. It will also 
have depth. Sometimes that refers to the number of bits in the image, 
but more frequently it refers to the number of channels. Thus we say 
that a grayscale image has a depth of 1, while a color image (with one 
channel each for red, green, and blue) has a depth of 3. 

We’ve seen in previous chapters that the word “depth” often refers to 
the number of layers in a neural network. 

Hence, two meanings of “depth,” and the opportunity for confusion. 

When we use “depth” with reference to a color image, it refers to the 
number of color channels. Most color images are represented by three 
numbers at every pixel, describing the amount of red, green, and blue 
light carried by that pixel, as in Figure 21.1. Thus we say that a color 
image has a depth of 3.

<img src='image/20.png'>

As we’ll see below, when an image passes through a convolution layer, 
it comes out as a tensor. In this context, “depth” refers to one of the 
dimensions of the tensor, or multi-dimensional block of data, at any 
given location in the network. 

Sometimes people use the term “fiber size” for the thickness of a tensor instead of “depth” to prevent confusion, but that usage is still 
infrequent. 

In general, when talking about a tensor, “depth” refers to the size of 
one of its dimensions. When talking about a network, “depth” refers to 
the number of layers.

# 21.2.2 Sum of Scaled Values

To kick off our discussion of convolution, let’s consider just a single 
pixel in a color image. As we’ve discussed, each pixel contains 3 numbers, one each for red, green, and blue. Suppose we want to determine 
if this pixel is yellow.

A pixel on a screen is displayed with light, and in that scenario colors 
combine additively (unlike pigments, which combine subtractively). 
Using light, we combine red and green to get yellow.

Let’s say that each of a pixel’s three primary colors is represented by a 
number from 0 to 1. So to test if a pixel is yellow, we want red (which 
we’ll abbreviate as just R) and green (G) to be nearly one, and blue (B) 
to be nearly 0. 

We’d like to ‘s make a single number that represents “yellowness.” The 
larger a pixel’s value of yellowness, the more yellow the pixel is. 
One way to measure yellowness is to find R+G−B for each pixel. Figure 
21.2 shows the value we get from this formula for eight different combinations of R, G, and B. The yellow pixel, with a score of 2, beats all 
the others that have scores −1, 0, and 1.

<img src='image/21.png'>

Another way to write R+G−B is to multiply red and green by +1, and 
blue by −1, and then add up the results: (1×R)+(1×G)+(−1×B). This may 
look familiar, because this little expression has the very same structure 
as the work carried out by an artificial neuron. We show that neuron in 
Figure 21.3. In this case, +1, +1, and −1 are the three weights, and the 
numbers associated with (R,G,B) are the three inputs. Each input is multiplied by its associated weight and the results are added together. 
Finishing the analogy, we need an activation function, so we’ll choose 
the linear function which has no effect.

<img src='image/22.png'>

Figure 21.3: Representing our yellow detector as a simple neuron. The 
pixel’s red and green values are inputs weighted with +1 , and the pixel’s 
blue value is weighted with −1 . The activation function is the identity 
function, shown here as a short diagonal line, which simply passes its 
input to its output without change.

We’ve just created a little artificial neuron tuned to detecting the “yellowness” of a pixel. 

Figure 21.4 shows how the process we just described can be performed 
across an entire image. We treat each pixel like a “core sample” drilled 
into the 3 combined channels of the image. We extract the “core sample,” and break it apart into three numbers, which become the inputs 
to the “yellowness” neuron.

<img src='image/23.png'>

Figure 21.4: A way to draw the operation of our neuron in Figure 21.3 for 
application over an entire image. Each pixel is extracted as a “core sample” 
of three values, which are used as inputs to the neuron. This operation, 
with identical weights, is repeated for every pixel in the image. The result 
is a new one-channel image where every pixel’s value represents the 
yellowness of its corresponding input pixel.

When we apply this neuron to all the pixels of an image, we often imagine the process as “scanning” the picture, moving the neuron from one 
pixel to the next, producing a new result pixel for each input pixel. This idea is shown in Figure 21.5.

<img src='image/24.png'>
Figure 21.5: One way to think of applying Figure 21.4 at every pixel is to 
imagine that we “scan” the original pixel left-to-right, top-to-bottom, and 
save the result in a new image.

If we run this neuron over each pixel of the image, one after the other, 
and save the output, we end up with a new image (with only one 
channel, since our neuron produces only one value) that tells us the 
“yellowness” of each pixel in the image, as in Figure 21.6.

<img src='image/25.png'>
Figure 21.6: An application of our yellow-finding operation. The image 
on the right runs from black to white, depending on the yellowness of 
the corresponding source pixel in the left image.

Of course there’s nothing special about yellow. We could build a little 
neuron that would assign the largest weight to any shade or color of 
our choosing, including subtle ones that are precise combinations of 
the primary colors.

# 21.2.3 Weight Sharing

In the last section we used just one yellowness neuron, which we swept 
over the entire image. 

In earlier discussions we imagined that each neuron’s weights were 
associated with its input wires, because that made them easier to name 
and discuss. But as we saw in Chapter 10, the weights are actually 
“inside” the neuron, or part of the neuron’s structure. For now, let’s 
return to thinking of them as belonging inside the neuron. So as we 
move the neuron over the image, it’s carrying its own weights with it. 

The upshot is that every pixel gets evaluated in exactly the same way, 
by the same neuron with the same weights. For each new pixel, only 
the input values, and thus the output value, change. 

Pretend for a moment that we wanted to do this yellow-finding process for the entire image as quickly as possible. Let’s suppose that the 
image is in a specific piece of memory, with the data for each pixel 
made available for us to connect to.

We might build a hardware version of our yellowness neuron, and then 
attach an identical copy of that hardware (including the weights) to 
every pixel in the image. We could then evaluate all of these neurons 
simultaneously, producing an entire image’s worth of “yellowness” 
categorization in the time it takes to run one neuron. 

Now suppose we wanted to detect a different color, say “magenta.” 
Because we’ve built the yellowness weights into the hardware neurons, 
we can’t re-use those circuits. We’d have to disconnect them, build new 
magenta-finding neurons, and wire them up to our pixels.

To save some time and effort, let’s put the weight information for 
each neuron into a piece of memory that we can both read and write. 
We’ll then place just one set of weights in some other memory somewhere. Let’s call these the shared weights. When we want to detect 
a color in our image, all the neurons could read the shared weights 
from that location and save them internally. Then they’d all use those 
same weights when evaluating pixels. This would let us apply all the 
neurons simultaneously, as before, but we can change the weights to 
all the neurons any time we like. So if we want to search for any other 
color, we just change the shared weights that get read, as we don’t have 
to re-wire anything. 

This is an entirely practical idea if we have some parallel hardware 
around (like a GPU). Using that, we can run multiple software copies 
of a given neuron in parallel, working on many pixels at the same time, 
as in Figure 21.7. Because we use the same weights at every pixel, the 
result of running these operations in parallel is identical to that resulting from scanning a single neuron over the image. It just comes out 
faster.

<img src='image/26.png'>

Figure 21.7: Using a parallel computer like a GPU, we can apply the same 
neuron to many pixels in the image simultaneously and independently. 
Here each circle with a multiply sign represents the operation of Figure 
21.4, where each of the pixel’s three values are multiplied by the corresponding weight, and the results added together.

When we use one set of weights for many copies of the same neuron, 
we call this weight sharing.

# 21.2.4 Local Receptive Field

So far, our one neuron that’s scanned over the image (or applied in 
parallel using weight sharing) is working with just one pixel at a time. 
We move the neuron to the pixel we want to process, read that pixel’s 
value and run it through the neuron to compute an output, then move 
the neuron to the next pixel and repeat the process. 

We’ll later see that we can connect our neurons to read several pixels 
at a time. Although these input pixels can take any shape, it’s almost 
always a square, and our neuron is at the center of the square.

For example, suppose that the neuron gets its input from a 3 by 3 
square of pixels (we’ll see how this is done in a moment). Then the 
pixel’s location is the center of the square, and the other 8 pixels are 
in a ring around it, as in Figure 21.8. In this example, our input image 
is grayscale, so there’s one value for each pixel. Thus our neuron takes 
in these 9 input values, multiplies each one by a corresponding weight, 
and sums the results. That single value goes into the output image in 
the same location as the highlighted pixel at the center of the square.

<img src='image/27.png'>

Figure 21.8: The neuron gets inputs in this case from a 3 by 3 square 
around the pixel. The location of the pixel in the image is highlighted in 
bright red. The set of all 9 pixels is called the local receptive field for the 
neuron. Our input image here has one channel, so the neuron receives 9 
inputs. Each is multiplied by a corresponding weight, shown in blue.

We say that the neuron has a local receptive field, meaning that the 
region (or “field”) from which it reads (or “receives”) values is small 
(“local”). We sometimes refer to the local receptive field more simply 
as the neuron’s footprint. The local receptive field is usually a little square that’s 1, 3, 5, or 7 pixels on a side, as in Figure 21.8, and centered under the neuron. Larger squares, and even other shapes, can be 
used as well. 

We know that the result of a neuron’s evaluation of the pixels in its 
local receptive field produces a single value, which goes into a new 
pixel in the output image. But specifically where in the output does 
that pixel go?

To answer this, we associate one of the element in the kernel as the 
anchor, or reference point, or zero point. This is highlighted in 
black in the center of the 3 by 3 grid in Figure 21.9. As we move the kernel over the image, the anchor moves with it. When we have a square 
kernel, we usually place the anchor in the center. We can say that the 
pixel that is under the anchor is the pixel that’s being evaluated. We 
can call this the focus pixel.

<img src='image/28.png'>

Figure 21.9: The neuron has a 3 by 3 receptive field, with the highlighted 
anchor at the center. The bright red pixel in the input image, called the 
focus pixel, shows where the anchor is currently located. The output of 
the neuron goes into the output image at the same location as the focus 
pixel.

As we think of moving a single neuron over the face of the input image, 
we can phrase this as moving the anchor of the receptive field from 
one focus pixel to the next. At each such pixel, the neuron evaluates 
the input values, produces an output value, and saves that in the output image at the same location as the focus pixel. Then it moves on to 
the next focus pixel.

This explains the popularity of footprints that are squares with an odd 
number of pixels on each side (often between 1 and 7). Those squares 
each have a pixel right in the center, which keeps everything simple 
and symmetrical.

# 21.2.5 The Kernel

When we think about a neuron as a single thing that’s moved across 
the image (or applied in parallel with shared weights), we often call its 
weights a kernel or filter. 

The word “kernel” comes from mathematics, where it has been used 
for a long time to refer to these values that form the conceptual core, or 
“kernel,” of operations like the one performed by our artificial neurons. 

The word “filter” comes from thinking about the neuron as manipulating, or “filtering,” input data. 
We sometimes extend the word “filter” to include the neuron itself, so 
we might say, “We move the filter over the image.” We also use it as a 
verb, so we might say, “The next step is to filter the image,” meaning 
that we’ll apply a specific set of weights to the image’s pixels.

# 21.3 Convolution

The name “convolutional neural networks” makes it pretty clear that 
“convolution” plays a big part in their operation. As we mentioned 
before, convolution is the name for a particular type of mathematical operation that involves two inputs. Although we won’t go into the 
mathematical formalities of convolution, we can summarize it as basically a carefully-choreographed combination of multiplication and 
addition. 

The good news is that we’re already familiar with convolution, because 
it’s what our artificial neurons have been doing all along, even though 
we haven’t described them in this particular way. 

Convolution starts with two lists of numbers of equal length. Using our 
existing language, let’s call one list the input values, and the other list 
the weights. We then multiply the first input and the first weight, the 
second input and the second weight, and so on. When all the multiplications are done, the results are added together, and that’s the result 
of the operation.

And that’s convolution. We’ll note in passing that we’ve left off 
a few details that belong to the formal definition of convolution 
[Oppenheim96]. That’s okay because those details don’t affect the big 
picture, so unless we’re programming the low-level algorithms directly, 
we can safely ignore them. 

Figure 21.10 shows this in action. Though we’ve moved the pieces 
around graphically, this is doing the same thing as our pictures of neurons that weight their inputs and sum the results.

<img src='image/29.png'>
Figure 21.10: A step of convolution involves two lists, which we can 
call “inputs” and “weights,” though both lists are treated the same way. 
Corresponding values are multiplied together pairwise, and then the 
results are added together. This is what an artificial neuron does, ignoring 
the activation function.

With just a little conceptual change, this idea suddenly becomes a powerful tool for working with images. 

The change is that instead of thinking of our inputs and weights as just 
simple lists, we think of them as grids of numbers (in fact, they can be 
tensors of any shape, but we’ll stick with grids for now). The two grids 
must be of the same shape. The operation is just the same as before: 
each element of the input grid is multiplied by the corresponding element of the weight grid, and the resulting values are then all added 
together. Figure 21.11 shows the idea.

<img src='image/30.png'>
Figure 21.11: Convolution in 2D works just like in 1D, though the picture 
gets more cluttered. Each pair of corresponding values gets multiplied 
together, and then all of those products are added together.

The reason why we say our change from a list of values to a grid is only 
conceptual is because we can always disassemble a grid into a list, and 
then use the list-based version of Figure 21.10. For instance, just make 
a new list that contains the first row of the input grid, followed by the 
second row, then the third row, and so on, as in Figure 21.12. We can 
then do the same thing for the weights, and then use our previous figure to multiply corresponding list entries.

<img src='image/31.png'>
Figure 21.12: We can think of 2D convolution as just 1D convolution if 
we first reshape each 2D grid into a single list. We just take the top row, 
append the second row, then append the third row, and so on. If we do 
this for both grids, we can just multiply the elements together pairwise 
and add up the result.

Let’s say we have a neuron has a filter composed of 9 weights, arranged 
in a 3 by 3 grid. As we discussed earlier, we can move this filter over 
the image. At each location we’ll have 9 pixels providing us with input 
values. We can multiply the two grids (or lists) together, add up the 
results, and thus have a value for that pixel in a new image. 

We call this convolving the filter with the image, meaning that we 
move the filter so that its anchor goes from one pixel to the next to the 
next, and at each point we gather up the pixel values in its footprint, 
multiply those with the corresponding weight in the kernel, and add 
up the results to produce an output.

Figure 21.13 shows the idea. In this image, we have a 3 by 3 filter sweeping over a 7 by 7 image. We haven’t discussed what might happen if 
the filter falls “off the edge” yet, so for now we’ll just limit ourselves to 
those locations where the filter sits entirely on top of the image. That 
means that the output image is only 5 by 5.

<img src='image/32.png'>

Figure 21.13: To convolve an image with a filter, we move the filter across 
the image and apply it at each position. The resulting value then becomes 
the value for that pixel in the result. Here are some positions of the filter 
in the input, and the positions where their computed values go into the 
output. Note that because the filter can’t extend past the edges, the input 
is 7 by 7 but the output is only 5 by 5.

Why would we want to do something like this? Let’s look at filters more 
closely.

# 21.3.1 Filters

Some scientists who study toads think that certain cells in the animal’s visual system are sensitive to specific types of visual patterns 
[Ewert85]. The idea is that the toad is looking for specific shapes that 
look like the creatures it likes to eat, and to certain motions that those 
animals make. 

People used to think that a toad’s eyes absorbed all that light that struck 
them, sent that mass of information to the brain, and then relied on the 
brain to sift among the results looking for food. The new hypothesis is that the cells in the eye are doing the initial detection all by themselves, 
and they only fire and pass on information to the brain if they “think” 
they’re looking at prey.

The idea has been extended to the human system, where it has been 
hypothesized that individual neurons fire in response to pictures of 
specific people. The original study that led to this suggestion included 
87 different images, including people, animals, and landmarks. In at 
least one volunteer they found a specific neuron that only fired when 
the visual system was presented with a photo of the actress Jennifer 
Aniston, leading to the idea of the so-called “Jennifer Aniston neuron” 
[Quiroga05]. Curiously, that neuron only fired when Aniston was alone, 
and not when she was pictured together with other famous actors. 

These ideas are not universally accepted [Sciffman01], but we’re not 
doing real neuroscience and biology here. We’re just looking for inspiration. And this seems like some pretty great inspiration.

The connection to convolutional layers is that we can use filters to simulate the toad’s eyes. The filters are the tools that pick out the patterns 
we’re looking for, and then pass on their discoveries to later layers 
which can then process that information. 

Some of the terminology that is used for this process uses terms that 
we’ve seen before. Specifically, we’ve been using the word “feature” to 
refer to one of the values contained in a sample, such as the temperature in a sample that contains multiple measurements of weather. But 
in this context, the word feature refers to the particular structure of an 
image that a filter is looking for. So we might say that a filter is looking 
for stripe feature, or a feature that looks like an eyeball. Continuing this 
usage, the filters themselves are sometimes called feature detectors.

Let’s see how feature detection works with a simple example. In Figure 
21.14 we show the process of applying a filter looking for short, isolated vertical white stripes to an image. Because the various pieces of 
this figure use different ranges of numbers, we’ve used different colors 
to show their values.

Figure 21.14(a) shows a 3 by 3 filter. The red cells show where the filter 
has a value of −1, and the yellow cells show where the filter has a value 
of 1. Figure 21.14(b) shows a noisy input image, ranging from 0 (black) 
to 1 (white). Figure 21.14(c) shows the result of applying the filter to 
each pixel in the input image (except for the outermost border). Here 
the values range from −6 (in purple) to +3 (in cyan). As we’ll see below, 
a score of +3 means that the filter and the image were a perfect match.
<img src='image/33.png'>

Figure 21.14: 2D pattern matching with convolution. (a): A 3 by 3 pattern 
of a column of 1’s (yellow), surrounded by −1’s (red). (b) A noisy input image 
of 0’s (black) and 1’s (white). (c) The output of the filter placed over every 
pixel in the image. The outputs run from −6 in purple to +3 in cyan. (d) 
A thresholded version of part (c), where pixels with a perfect score of +3 
are in white, and others are in black. (e) The input image of part (b), but 
the 3 by 3 block around each white pixel in part (d) is highlighted.

Figure 21.14(d) shows a thresholded version of Figure 21.14(c), where 
pixels with a value of +3 are shown in white, and all others are black. 
Finally, Figure 21.14(e) shows the noisy image of Figure 21.14(b) with 
the 3 by 3 grid of pixels around the white pixels in Figure 21.14(d) 
highlighted. We can see that the filter found those places in the image 
where the pixels matched the filter’s pattern. 
Let’s see why this worked. In the top row of Figure 21.15 we’ve shown 
our filter and a 3 by 3 patch of the image, along with the pixel by pixel 
results. In this situation, only one of the white pixels (the top center) is 
matched by a 1 in the filter. This gives a result of 1×1=1. The others are 
matched up with −1, giving results of −1×1=−1. The zeros in the image 
are irrelevant, since whether we multiply them by 1 or −1 we still get 
back 0. Adding up the −1 score for the three white pixels in the corners 
with the 1 score for the white pixel in the top center gives us −3+1=−2.

<img src='image/34.png'>

In the lower row, our image matches the filter. All three white pixels 
contribute 1, and there are no out-of-place white pixels that would pull 
the score down by contributing −1. There are also no missing white 
pixels, which would also bring our score down by not adding 1. The 
result is a score of 3, indicating a perfect match. 

This process is a close match to our yellow-finding neuron of Figure 
21.3, except here we’re using only one weight per pixel, and our weights 
are spread out over multiple pixels. 

Figure 21.16 shows another filter, this one looking for diagonals. We’ll 
run it over the same image. This diagonal of 3 white pixels surrounded 
by black is present in exactly one place in our random image, near the 
lower-left corner.

<img src='image/35.png'>
Figure 21.16: Another filter and its result on our random image. The three 
diagonal white dots surrounded by black are only found in only one place.

So by moving the filter over the image and measuring the final value at 
each pixel, we can hunt for the patterns we’re looking for. With larger 
filters that contain more nuanced values than simply +1 and −1, we 
can make much more complex patterns to find more interesting features. We can even perform image processing operations like blurring 
and sharpening the image [Snavely13]. 

If we take the output of a first set of filters and feed them to another set, 
we can look for patterns of patterns. If we feed that second set of outputs to a third set of filters, now we’re looking for patterns of patterns 
of patterns. We can repeat this many times, creating a deep hierarchy 
of filters. Perhaps surprisingly, such a hierarchy allows us to look for 
complex features of any orientation or size, whether it’s the face of a 
friend, the grain of a basketball, or the eye on the end of a peacock’s 
feather.

If we had to work out these filters by hand, classifying images would 
be tedious at best. What are the proper filters in a hierarchy 8 levels 
deep that will tell us if a picture is that of a kitten or an airplane? How 
would we even go about working out that problem? And how would 
we know when we had the best filters? 

The beauty of CNNs is that we don’t have to figure out which filters we 
need, because the computer does it for us.

The learning process that we’ve seen in previous chapaters, involving 
measuring error and then improving weights with backprop, teaches a 
CNN to find the best filters. It modifies the weights, (that is, the values in the kernel) of each filter, until the network is producing results that 
match our targets. In other words, it tunes the values in the filters until 
they find the features that enable it to come up with the right answer. 
And it can do this for hundreds or even thousands of filters, all at once. 

It can seem almost magical that this can work, but the training is basically just standard backprop and gradient descent. At the broadest 
level, we just modify each weight in each kernel in such a way that it 
follows the downhill error gradient and thus brings down the overall 
error. Do this enough times, and the weights will form filters that give 
us all the information we need to transform an input into an output 
that matches the label.

# 21.3.2 A Fly’s-Eye View

Let’s look at an alternative way to visualize this whole process. Instead 
of sliding a filter over an image, imagine taking a photograph and 
breaking it into many small, overlapping pieces, each the size of the filter. Then we plunk the filter over each piece and perform the filtering 
operation as before, multiplying each pixel value by its corresponding 
filter value [Geitgey16]. Figure 21.17 shows the little pieces of the input 
image that we’d then place the filter on top of.

<img src='image/36.png'>
Figure 21.17: Rather than thinking of sweeping the filter across the 
image, we can imagine breaking the image up into overlapping pieces, 
like a fly’s-eye view of the image. Then we apply the filter to each piece, 
producing the output values (image after [Geitgey16]) .

In this view of convolution, we place the filter over each little overlapping piece of the input image, rather than sweeping it over the original 
image. 

Both ways of thinking about this yield the same result.

# 21.3.3 Hierarchies of Filters

We’ve taken inspiration from biology already in this chapter, and we 
can do it again. 

Many real visual systems seem to be arranged hierarchically
[Serre14]. In broad terms, we think of the processing in the visual system as taking place in a series of layers, with each successive layer 
working at a higher level of abstraction than the one before. Returning 
to the visual system of a toad, the bottom-most layer might be looking 
for “light-colored blobs,” the next “combinations of things from the 
previous layer that also have wings,” the next “combinations of things 
from the previous layer that are moving in short fast bursts,” and so 
on up to the top layer which looks for “flies” (these features are completely imaginary and used just as illustrations of the idea).

This approach is nice conceptually, because it lets us structure our 
analysis of an image in terms of a hierarchy of image elements, and the 
filters that look for them. It’s also nice for implementations, because 
it’s a flexible and efficient way to analyze an image. 

Let’s look at a simplified example to get a feeling for the process. We’ll 
try to find a face in a 27 by 27 binary image. Let’s suppose we want to 
find the face on the left of Figure 21.18, but our input image contains 
all kinds of other stuff, shown on the right of Figure 21.18. Since we 
know the exact locations of all the pixels we’re interested in, we could 
just check for their presence directly. But let’s see how to solve this 
problem using a hierarchy of convolution filters, because that more 
general approach is what we’ll use later to find objects even in complex 
color photographs.

<img src='image/37.png'>
Figure 21.18: Finding a face in a 27 by 27 binary image. Left: The face we’d 
like to detect. Right: The input image we’re given.

The overall flow of our strategy is shown in Figure 21.19. We’ll begin 
on Layer 1 by applying five little filters, each one just 3 by 3. Our first 
filter will look for 3 black elements in a horizontal row surrounded by 
white elements above and below. We’ll call that filter H, for horizontal. 
We’ll make a similar filter for a vertical stripe and call the filter V. We’ll 
also look for solid 3 by 3 blocks, and call that filter B. To include a bit 
of detail, we’ll also look for left and right ends of horizontal lines as a 
single black pixel surrounded by white pixels, and we’ll call those filters L and R. These five filters make up our first layer.

<img src='image/38.png'>
Figure 21.19: Using 3 layers of filters, each filter only 3 by 3 pixels, to find 
a forward or profile face in a 27 by 27 image.

After these filters have run, we’ll use max pooling to reduce their output dimensions by 3. That is, we’ll consider the output of each filter as 
a set of non-overlapping 3 by 3 blocks. Any time we find a block that contains a cell that matched that filter, we’ll mark in black the corresponding cell in the lower-resolution output. So each of the five filters 
produces an output that’s 27 by 27, which we then reduce to 9 by 9. 

We’re reached Layer 2, where we’ll apply three more filters, each also 
of size 3 by 3. These higher-level filters will examine the 9 by 9 images 
coming out of Layer 1. They’re looking for combinations of our 5 lowest-level building blocks that make up pieces of our face. We’ll mark 
each entry in these filters with the building block we want, or an X if 
we don’t care what’s in that cell. We’ll make a filter for the nose, which 
is just a little vertical line over a horizontal line (we’ll call that filter N), 
a filter for an eye, which is a block of pixels under an eyebrow made up 
of a horizontal line with left and right ends (we’ll call that filter E), and 
a filter for the mouth, a long horizontal line with a shorter horizontal 
line under it (we’ll call that M). Once again we’ll apply pooling, so each 
filter’s final 9 by 9 output is reduced to 3 by 3.

Finally, we reach Layer 3. Here we’ll apply 2 new filters, again each 3 
by 3. One will look for either a face looking forward (we’ll call that F), 
another for a face in profile to the right (we’ll call that P). Our profile 
image would look pretty weird, but that’s okay for this demonstration. 

Let’s see these filters in action. Figure 21.20 shows the H, V, and B filters running over the original image. Since we’re convolving our filters 
with the image, we’ll move the center of each filter so it’s over each 
pixel in the input, and determine if there’s a match (that is, the white 
and black pixels in the filter match the white and black pixels in the 
image). Each pixel that matches will get outlined in red.

<img src='image/39.png'>
Figure 21.20: Applying filters H, V, and B to our original 27 by 27 image. 
Top row: The filters we’re using. Middle row: Each pixel that matches the 
filter is highlighted in red. We also show the 3 by 3 blocks that are used 
for max pooling. Any block with a red cell has been highlighted with a 
thicker outline. Bottom row: The result of the pooling operation. 3 by 3 
cells that were highlighted in the middle row are marked in black.

After we’ve found our matches, we’ll use max pooling with 3 by 3 
blocks to reduce the size of our input image, from 27 by 27 to 9 by 9. 
Any block that has at least one match inside becomes black. Note that 
some blocks have more than one match, such as near the middle-left 
of the V filter. We don’t do anything special for those blocks, but just 
mark them in black like any other block that contains a match. 

The results for the L and R filters are shown in Figure 21.21.

<img src='image/40.png'>
Figure 21.21: Applying the L and R filters to our input face, using the 
same conventions as in Figure 21.20.

Now it’s time for filter level 2. In Figure 21.22, we’ve combined all 5 
of the output images into one diagram where each cell is marked 
with one or more letters, so the filter outputs are easier to take in as a 
group. In our simple example, most blocks had only one match with 
any of the filters, though the block just to the lower right of the center 
was matched by both the L and R filters. We apply our eye, nose, and 
mouth filters E, N, and M just as before, but this time we won’t demand 
that all the values match. Recall that an X in a cell means “don’t care”, 
which we can think of as a kind of wild card that matches everything. 
For example, the upper-right 3 by 3 block in the composite diagram 
almost matches the E filter, but there’s an extra R in the bottom right. 
Since that cell has an X in the corresponding entry in the filter, that 
block still matches when the E filter is centered over the B.

<img src='image/41.png'>

Figure 21.22: Top row: Our three second-layer filters. Second row: The 
summary of the first layer. Each cell is marked with the filter that matched 
it, if any did. Third row: The E, N, and M filters are moved over the image 
in steps of 3. Cells they matched are shown in black. Bottom row: The 
output of each filter is a new image, now only 3 by 3.

As before, we’ll apply max pooling using 3 by 3 blocks, so our output 
from this layer consists of 3 images, each 3 by 3. 
Now we’re ready for the third and final layer, which tells us if the input 
image contained a forward-looking face, a face in profile, or neither. 
We just apply the two 3 by 3 filters to the results of the second layer, as 
in Figure 21.23. There’s no need to move the filters around, since the 
inputs are 3 by 3. Here, the forward looking face matched, while the 
profile did not.

<img src='image/42.png'>
Figure 21.23: Applying our third-level filters to the output of the second 
level, using the same layout as in previous figures. The forward filter 
matches, while the profile filter does not.

We’ve detected our face in the presence of all kinds of other distracting 
objects in the image! 

All we had to do was design our 10 little filters, and we could match 
our face, even when there was lots of other stuff in there to distract us. 
If we wanted to look for a different type of nose, we could just redesign the nose filter. Or we could look for many kinds of facial features 
at once using a filter for each type of eye, nose, and mouth, and then 
make higher-level filters that match various combinations to help us 
tell which kind of face we’ve been given. 

If we had to design these filters by hand for every project this wouldn’t 
be a very attractive algorithm. But a deep learning system can automatically learn the best values for the filters from the inputs after it’s 
been exposed to many labeled examples during training.

Our example used a binary image so that it was easier to see what was 
happening, but in practice we often use grayscale and color images. 
In these cases, our filters will contain floating-point values. While the black and white filters could only report a match or the lack of it, these 
floating point filters can return a floating point number, where a larger 
number means it made a better match with the data at that location. 

A great thing about convolution is that the filters are able to find what 
they’re looking for anywhere in the image. For example, the H filter 
found all horizontal runs of 3 black pixels with white pixels above and 
below, everywhere in the image. When we’re looking to process complex photographs of the natural world, this gives us the flexibility to 
detect image features robustly, even if they’re not exactly where we 
expect them. 

There’s a sense in which our filters are getting more powerful as we 
work our way up the levels. Consider that a 3 by 3 filter on the second 
level is effectively responding to a 9 by 9 region, since each of its pixels is the result of the previous step which reduced the image size. For 
example, our eye filter E is processing a 9 by 9 region, though it’s only 
3 by 3 itself. In this way, the filters at higher levels in a hierarchy are 
able to look for large and complex features, even though they use only 
small (and therefore fast) filters.

Higher levels are able to combine the results of lower levels in multiple ways. Suppose we want to classify a variety of different birds in a 
photo. Low-level filters might look for feathers or beaks, while higher 
filters are able to combine different types of feathers or beaks to recognize different species of birds, all in a single pass through a photo. 
This technique is sometimes referred to as working with a hierarchy 
of scales.

# 21.3.4 Padding

Let’s return to convolution and look at what happens near the edges of 
an input.

Suppose that we want to apply a 5 by 5 filter to a black-and-white 
image. If we’re somewhere in the middle of the image, as in Figure 
21.24, then our job is easy. We pull out the 25 values from the image, 
scale them by the 25 values in the filter, and sum up the result.
<img src='image/43.png'>

But what if we’re right on an edge, as in Figure 21.25?
<img src='image/44.png'>

The footprint of the filter is hanging off the edge of the image. There 
aren’t any input values there. How do we compute an output value for 
the filter when it’s missing some of its inputs? 

We have a few choices. One is to disallow this case, so we can only place 
the footprint where it is entirely within the input image, as we’ve been 
doing so far. Any pixels where we can’t place the filter get dropped 
from the output, making it smaller in each dimension. Figure 21.26 
shows this idea.

<img src='image/45.png'>
Figure 21.26: We can avoid the “falling off the edge” problem by never 
letting our filter get that far. With a 5 by 5 filter, we can only center the 
filter over the pixels marked here in blue. The resulting 6 by 6 output, 
shown on the right, is smaller than the 10 by 10 grid we started with.

While simple, this is a lousy solution, because if we apply multiple 
convolution filters to the same image, it will shrink on every step. We 
could end up with just a very small piece of the image as our input, 
which isn’t a good result. 

A popular alternative is to use padding. The idea is that we add a 
border of “extra” pixels around the outside of the image, as in Figure 
21.27. All of these pixels have the same value. By far the most common 
choices is simply zero. This choice is called zero-padding.

<img src='image/46.png'>
Figure 21.27: A better way to solve the “falling off the edge” problem is to 
add padding, or extra pixels, around the border of the image. Here we’ve 
added a 2-pixel border, so every pixel in the original image (shown in 
white) can be used as the center of the filter. Usually, the padded pixels 
are given the value 0.

The size of the border depends the size of the filter. We usually use just 
enough padding so that the filter can be centered on every pixel in the 
original image. Libraries might offer automatic calculation of the padding size based on the filter size, or they might require us to specify it 
manually. Normally we never explicitly place padding into our input 
images. Instead, we leave it to the library to create (or presume) these 
pixels when they’re needed. 

Using padding, we can create an output image of the same size as the 
input.

# 21.3.5 Stride

When we sweep a filter over an image, we can imagine it moving the 
same way we read a book. For the moment, let’s assume we’re using 
padding. The filter will start in the upper-left pixel of the input image, 
produce an output, then take one step right, produce an output, move 
another step right, and so on until it reaches the right edge of that line. 
Then it moves down one line and back to the left side, and the process 
repeats. 

But we don’t have to be so methodical. Suppose we move, or stride, 
more than one pixel to the right, or more than one pixel down, as we 
sweep our filter? Then our output will end up being smaller than the 
input. 

We’ll see that there are a few different ways to use striding, all of which 
are important when we use convolution layers. 

To visualize striding, let’s think of our output as initially a blank slate. 
As the filter moves left to right, it produces a series of outputs, and 
those get placed one after the other, also left to right, in the output. 
When the filter moves down, the new outputs is produces go on a new 
line of cells in the output. 

If we move, or stride, 1 pixel in each direction, as we’ve been doing 
so far, we get the results shown in Figure 21.28. Here we’ve left off 
padding.

<img src='image/47.png'>
Figure 21.28: Sweeping a 3 by 3 filter over a 5 by 5 image, without padding. 
Each step of the filter moves it to the right by one pixel in the input, and 
then it moves down by one pixel. The diagram shows 4 of the 9 filter 
positions that would make up this output.

But we could skip over pixels horizontally, or vertically, or both. For 
instance, we might move to the right by 3 pixels on each horizontal 
step, and then move down by 2 lines on each vertical step. The output 
pixels are still assembled as before. The result is a new image that is 
one-third the size of the original horizontally, and one-half the size 
vertically. This is shown in Figure 21.29.

<img src='image/48.png'>
Figure 21.29: Our input scanning can skip over pixels. In this example, 
we look only at every third pixel horizontally, and every other pixel vertically. In other words, we use a stride of 3 horizontally and 2 vertically. 
For clarity, we’re not using padding in this figure. The 5 by 9 input image 
turns into a 2 by 3 output.

Another look at the pixels that served as locations for the filter’s anchor 
in Figure 21.29 is shown in Figure 21.30.
<img src='image/49.png'>
Figure 21.30: The six pixels where the filter was anchored in the convolution of Figure 21.29.

We’ll see later that this is a fast way to reduce the size of the input 
image, in order to speed up later blocks in the network. 

In Figure 21.29 we used a stride of 3 horizontally, and a stride of 2 vertically. More often we specify a single stride value for both axes. The 
stride can be any positive integer, starting with 1. The default stride, as 
shown in Figure 21.28, is 1, meaning that we step 1 pixel on each move, 
and none are missed. A stride of 2 on both axes can be thought of as 
taking every other pixel both horizontally and vertically, and similar 
thinking holds for a stride of 3 or more. Pictures of these two sets of 
strides are shown in Figure 21.31
<img src='image/50.png'>

__stride: là cách chúng ta muốn di chuyển bộ lọc mỗi lần bao nhiêu ô theo hàng ngang, bao nhiêu ô theo hàng dọc__
VD: (1,1): đầu tiên là ô (0,0), sau đó di chuyển 1 ô ngan.

We can use striding to prevent filters from overlapping, which is useful 
when we want to shrink an image. For instance, if we’re moving a 3 by 
3 filter over an image, we might use a stride of 3 so that no pixel gets 
used more than once, as in Figure 21.32.

<img src='image/51.png'>

Figure 21.32: Using a stride of 3 in each dimension with a filter of size 3 
by 3 means that each pixel in the input will be used only once. Read the 
figure top-down, left-to-right. Pixels shaded in gray have already been 
covered by the filter. The output image will be one-third the size of the 
original in each dimension.

# 21.4 High-Dimensional Convolution

In our examples above we’ve been working with black-and-white 
images. That is, each image has only one channel of color information. 
We know that color images have at least 3 channels, most usually representing the red, green, and blue components of each pixel. Let’s see 
how to handle those. Once we can work with images with 3 layers, we’ll 
also know how to work with tensors of any number of layers, such as 
the outputs of previous convolution layers. 

One way to handle a color image is to apply the same filter to each 
channel. Alternatively, we could make a filter that applies different 
weights to each channel.

It’s easy to make such a filter. We turn our previous filter, which was 
just a grid of weights, into a stack of filters, one layer for each channel. 
Figure 21.33 shows the idea. In other words, our kernel moves from a 
2D grid to a 3D block that contains 27 values.

<img src='image/52.png'>

Figure 21.33: When we apply a filter to a three-channel image, such as 
a color picture made up of red, green, and blue values at each pixel, we 
can apply three different filters, one for each channel. We can picture the 
three filters as a little stack, shown here for filters with a 3 by 3 receptive 
field.

To apply this kernel to a 3-channel color image, we proceed much as 
before, but now we think in terms of blocks (or tensors of 3 dimensions) rather than grids (or tensors of 2 dimensions). 

Returning to our earlier idea of a “core sample,” let’s suppose our filter 
has a 3 by 3 footprint, and we’re going to process an RGB image with 
3 color channels. So we pull out a “core sample” from the RGB image, 
but now it’s a volume that is 3 pixels on each side (3 for height and 
width, because the filter is 3 by 3, and 3 for depth, because there are 3 
channels). 

Then we multiply each element of the block with the corresponding 
element in the block representing the filter.

If we want to produce a single value representing how closely the RGB 
image matched the tensor of the kernel, we could add up all 27 multiplied values and produce a single value for a new output image with 
just one channel, as in Figure 21.34.

<img src='image/53.png'>

Figure 21.34: Convolving an RGB image with a 3 by 3 by 3 kernel. We pull 
out the 9 red, green, and blue pixel values for the 3 by 3 footprint, and 
multiply those elements with the corresponding slice of the kernel. We 
can add up all the results to produce a single value.

Let’s suppose that we wanted to produce an RGB image as output, but 
with each color channel modified by its own filter. Then instead of using 
one 3D kernel, we can use three separate 2D grids. Then each filter is 
applied to its own channel, and produces its own output. The result is 
a new tensor with 3 channels, one from each filter. Figure 21.35 shows 
this approach.
<img src='image/54.png'>

These ideas can be extended to images with any number of channels, 
as we’ll see in the next section. 

We might choose to increase the efficiency of our implementation by 
applying many identical filter kernels simultaneously using parallel 
hardware. In that case, we would use the same weight sharing we saw 
before, meaning that all the filters still get their weights from a single 
source.

# 21.4.1 Filters with Multiple Channels

We can generalize the ideas above to use multiple filters on a single 
input. 

For example, suppose we have a black-and-white image, and we want 
to look for eyeballs, baseballs, volleyballs, and meatballs. We could 
create one filter for each of these features, and run each one over the 
input independently. The result would be four output images, each one 
channel deep, one from each filter. Figure 21.36 shows the idea.

<img src='image/55.png'>

So instead of producing a grayscale image with one channel, or a color 
image with three channels, we now have an image with four channels. 
If we used 7 filters, then the output would be a new image with 7 channels. At that point we’d probably want to stop calling it an image and 
refer to it more generally as a tensor. 

The key thing to note here is that each filter has 1 slice, matching the 
input. If the input was 2 pixels deep, each filter would need to also be 
2 pixels deep.

Generally speaking, our filters can have any footprint, and we can apply 
as many of them as we like to any input image. What’s most important is that the number of channels in the filter matches the number of 
channels in the input. 

Figure 21.37 shows this idea. The input tensor at the far left has 7 channels. We’re applying four different filters, each with a 3 by 3 footprint, 
so each filter is a tensor of size 3 by 3 by 7. The output of each filter 
goes into its own output which is a single channel deep. Since we’re 
applying 4 filters, the output tensor is 4 channels deep.
<img src='image/56.png'>

Figure 21.37: When we convolve filters with an input, each filter must 
have as many slices as the input. Here the input is 6 channels deep, so 
each filter is 6 channels deep. The 4 filters each create an output of 1 
channel, so the final output has 4 channels.

Although in principle each filter we apply can have a different footprint, 
in practice we almost always use the same footprint for every filter in 
any given convolution layer. For example, in Figure 21.37 all the filters 
had a footprint of 3 by 3. If another convolution layer follows this one, 
the filters on that new layer could have a footprint of any size. 

We’ll see below how convolution layers manage all of this accounting 
for us. Jumping ahead a little, to add the entire operation of Figure 
21.37 to our network, all we need to specify is that we want 4 filters, 
each with a footprint of 3 by 3 and a depth of 6. If the library can automatically match the input tensor’s depth, it will make us 4 filters 
of size 3 by 3 by 6, and initialize them with random values. During the 
forward pass the filters will all be applied and the results combined 
into the 4-channel output. During backpropagation, the 54 values 
(3×3×6) in each filter will get adjusted to improve the error at the network’s output.

# 24.4.2 Striding for Hierarchies

We saw earlier that using a series of convolutions on an image of 
ever-decreasing size lets us efficiently look for objects made up of lots 
of elements. Striding makes this easy, because it inherently allows us 
to output an image that’s smaller than the input. For example, if the 
stride is 2 units in some dimension, the output will be 1/2 the size in 
that dimension. If the stride is 3 units, the output in that dimension 
will be 1/3 the size, and so on. 

Let’s suppose we start with a 600 by 600 black and white image, as in 
Figure 21.38 Our first convolution layer will apply 8 filters, each of size 
5 by 5, to this 600 by 600 image. We’ll pad the image with 2 rows of 
0’s all around so the image won’t get smaller, but we’ll use 2 steps of 
stride. This means we’ll get back an image of half size, or 300 by 300, 
with 8 channels.

<img src='image/57.png'>
Figure 21.38: Creating a hierarchy of convolutions. The first convolution 
works on the full 600 by 600 image. The second works on a 300 by 300 
version of that image that contains the results of the first stage’s 5 filters. 
Each stage uses a lower-resolution version of the previous stage, so it can 
work with larger collections of features without requiring larger filters.

Now we’ll run 4 new filters over this image, each filter 3 by 3, with 4 
channels. We’ll use a stride of 3 and 2 steps of padding, getting back a 
tensor size 100 by 100 by 4. 

Because the image has been scaled down at each step, each succeeding 
set of filters is working with a lower-resolution version of the original. 
This means they run faster, and can look for larger features, since a 3 
by 3 filter on a 300 by 300 image can be thought of roughly as a 6 by 6 
filter on the previous layer’s 600 by 600 image. 

Conceptually, one of the filters on our first convolution might be 
implementing the toad’s hypothetical “looking for light-colored blobs,” 
while another filter is looking for “things with wings.” Then the next 
convolution can look at these two results simultaneously, and look for 
“blobs that have wings.” 

We can keep up this shrinking until we end up with a tensor of just 
one pixel on a side, though that’s rare in practice. The only rule is, as 
we saw above, that the filters at each step must have one slice for each 
channel in the image that they work on.

In this example, we’ve reduced the resolution of our image, sometimes 
called downsampling, using striding during convolution. An alternative is to do no striding (that is, move just 1 pixel horizontally and 
vertically as we move the filters), and the follow the operation with a 
pooling layer. In recent years, experience has shown that doing the 
downsampling with striding while convolving is faster and often gives 
results that are just as good, or better, so it’s become the more common 
idiom [Springenberg15]. Nevertheless, pooling was used by many popular architectures that are still in common use today, so it’s important 
to be familiar with the technique. 

We’ll see that we often run this process in reverse as well, increasing
the number of pixels in an image, say from 100 on a side to 300 on a 
side. We can do this process, called upsampling, with an upsampling 
layer, as we discussed in Chapter 20. But just as with downsampling, 
we can do this increase in resolution while computing the convolution 
itself. We’ll see how that’s done later in this chapter.

# 24.5 1D Convolution

In our previous discussions we’ve discussed moving our kernel in 
two dimensions, both horizontally and vertically across our 3D input 
image (height, width, and one or more channels). As we discussed, a 
grayscale image has 1 channel, an RGB image has 3 channels, a CMYK 
image has 4 channels, and other color representations may have other 
numbers of channels. Recall that we don’t move the filter in depth, 
because the filter itself has as many channels as the input image. 

What if the input isn’t 3D? We can generalize our discussion to both 
smaller and larger tensors, which involves moving our tensor through 
fewer and more dimensions, respectively. 

An interesting special case is called 1D convolution. This involves 
moving the filter in only one direction [Snavely13]. This is a popular 
technique when working with text, where each row represents a single 
word, or a fixed number of letters [Britz15].

The basic idea is shown in Figure 21.39. We create one or more filters that are the entire width of an input matrix, where we can place 
one word of our text in each row. Once we’ve computed an output for 
each filter, we move it one row downwards. The name “1D convolution” 
comes from this single direction, or dimension, of movement.

<img src='image/58.png'>

As we saw before, we can have multiple filters sliding down the grid, 
each of a different height. 

Any time we move our filter in just one dimension, we can call that “1D 
convolution.” It’s easiest to see with a 2D grid, but we can do the same 
thing with an input of any number of dimensions, as long as we only 
slide the filter along the input in one dimension. 

Because of its name, 1D convolution can be easily confused with convolution with a 1 by 1 filter, often called 1 by 1 convolution, or 1×1 
convolution. These two ideas are very different. Let’s look at 1×1 
convolution now.

# 24.6 1×1 Convolutions

We’ve seen how to use multiple filters to create multiple outputs. And 
then future steps of convolution can use new filters that take the output of those filters as input. 

But what if we just want to combine the filter outputs in some way, 
without using a big footprint? 

For example, we might have one filter that spots red circles, and 
another that finds green lines, and we’d like to find those pixels with 
both a red circle and a green line (such pixels would be yellow). 

We can build a 1 by 1 filter, often written as a 1×1 filter, and use that 
to perform 1 by 1 convolution [Lin14]. 

This is a filter with a footprint of just one pixel. It’s normal convolution, 
in that we sweep this filter over the input tensor, multiply the values in 
the input by the weights in the filter, produce a result, and save that in 
a new output tensor. The only difference is that the filter’s footprint is 
just a single pixel in both dimensions. Figure 21.40 shows this visually.

<img src='image/59.png'>
What’s the point of such a tiny filter? One powerful application of 1×1 
convolution is to do feature reduction on the fly. This is a familiar 
idea: in Chapter 12 we saw how to use algorithms like PCA to pre-process our data to reduce the number of features, and thus improve the 
performance of our algorithms. In this case, our 1 by 1 filter is specifying a way to project all of the values at an elment down onto a line, 
where we need only a single value to represent it. 

The value of such an operation is two-fold. First, there’s less data to 
process, so further calculations go faster and consume less memory. 
Second, the network is often able to produce better results because it 
can direct all of its computational power on useful information, rather 
than wasting it on redundant features. 

Let’s see how 1×1 convolution can pull off this trick for us. Suppose 
we start with a tensor that has 300 layers, as in Figure 21.41. We suspect that lots of the data in those layers is redundant, and we think we 
could get good results with just 175 layers.

<img src='image/60.png'>

We can make 175 filters, each with a 1×1 footprint. Each filter will look 
at all 300 values located at one pixel, and produce just a single value as 
a result. With training, each of these filters can extract one useful measurement from those 300 incoming values. The result is a tensor with 
only 175 layers, so all following operations will go nearly twice as fast. 
If our guess of 175 was right, then we could still get acceptable results 
from our network, but with less time and memory consumption. 

This process often works well in practice when the features are correlated [Canziani16]. That means that the filters on the previous 
layers have created results that are in sync with one another, so that 
when one goes up, we can predict by how much the others will go up 
or down. The better this correlation, the more likely it is that we can 
remove some the correlated layers and suffer little to no loss of information. The 1 by 1 filters are perfect for this job. 

Inserting layers that perform 1 by 1 convolutions into our convnets of 
many layers can improve their performance, as demonstrated by the 
architecture given the colorful name “Inception” [Szegedy14].

# 24.7 A Convolution Layer

We’ve talked a lot about convolution layers without saying much 
about how they work. Let’s address this now. 

When we create a convolution layer, we typically tell the library 
how many filters we want, what their footprint should be, and other 
optional details like whether we want to use striding and what activation function we want to use, and the library takes care of all the 
rest. Most importantly, it improves the kernel values in each filter with 
backpropagation, learning the best values that make filters that produce the best results. 

The output of a convolution layer is a tensor, with one slice for each 
filter, as we’ve seen above. Because each filter is designed to match a 
feature in the input, the output of a convolution layer is sometimes 
called a feature map. The word “map” comes from its mathematical meaning. Here we can think of this “map” as telling us where each 
of the features are in its input, with larger values indicating increased 
likelihoods.

When we draw a diagram of our model, we usually identify our convolution layers by how many filters are used, their footprints, and the 
activation function. Usually the default is to apply no padding and use 
a striding of (1,1), so if we want different values for these options we 
include them explicitly. Since it’s common to use the same padding all 
around the input, we often just provide a single value rather than two, 
with the understanding that it applies to all dimensions. So a padding 
of 3 would stand for (3,3) in 2D, or (3,3,3) in 3D. 

Some libraries will automatically compute the amount of padding 
that’s needed in order to keep the output the same size as the input, so 
all we have to do is ask for padding, rather than tell it explicitly how 
much to use. Figure 21.42 shows our shorthand icon for two convolution layers, along with the traditional box-and-text versions.

<img src='image/61.png'>
Figure 21.42: Two convolution layers. Each is shown in our schematic form, 
and traditional box-and-label form. (a) A convolution layer with 5 filters, 
each 3 by 3, and an ReLU activation function. The stride is implicitly (1,1), 
and there’s no padding. (b) The same layer as in part (a), but now with an 
explicit stride of (3,3), and a single ring of zero-padding around the input.

# 24.7.1 Initializing the Filter Weights

When we create a new convolution layer, all the necessary filters are 
also created. We haven’t learned anything yet, so we don’t know what 
weights should be in the filter kernels. But just like the weights for regular artificial neurons, they have to be initialized with something. 

If two filters have identical weights, we’re wasting resources, since 
both would do the same job. We say that two such filters are symmetrical. We want to prevent such filters from forming, so we definitely 
don’t want to initialize two filters with the same values. For this reason, assigning different values to every filter is called symmetry 
breaking. 

One approach to initialization with symmetry breaking is to use small 
random numbers, such as those from the range [−0.01,0.01]. This 
makes it unlikely that any filter will exactly duplicate any other filter.

Research into initialization has led to other approaches. Two popular techniques are both named for the lead author on the paper that 
described them. These are Glorot initialization (also called Xavier 
initialization) [Glorot10] and He initialization [He15]. Recall that 
we saw these different initializers in Chapter 16.

Both of these techniques work on the same general idea: the initial values are random numbers that are chosen according to the number of 
inputs to the neuron. This is called the fan-in. Both Glorot and He 
choose random values from a distribution that uses the fan-in as a 
parameter [Jones15]. 

A nice feature of these approaches is that they take no other parameters, and in particular no user-specified parameters. We merely need 
to tell our library which initialization we want, and it takes care of the 
rest. Without going into the theory, the current recommendation is 
that if our library provides He initialization as an option, we should 
use it [Karpathy16]. Otherwise, Glorot or random values can be used 
instead.

# 24.8 Transposed Convolution

The convolution layers we’ve looked at so far either maintain the size 
of their input or make it smaller. But we can use the same technique to 
also make the input tensor larger. This process is called upsampling. 
When we do it inside of a convolution step, it’s called transposed 
convolution or fractional striding. The word “transposed” comes 
from the mathematical operation of transposition, which we can use to 
write the equation for this operation. We’ll see where “fractional striding” comes from below. 

Some authors call upsampling while convolving deconvolution, but 
that name is already taken by a different idea [Zeiler10]. To avoid confusion, most people avoid that term now, preferring either of the other 
terms above. In this chapter, we’ll use the term transposed convolution. 

Let’s see how this works. We’ll begin by revisiting basic convolution 
without padding, creating an output smaller than the input. In Figure 
21.43 we move a 3 by 3 filter over a 5 by 5 input, resulting in a 3 by 3 
output.

<img src='image/62.png'>
Figure 21.43: Convolving a 5 by 5 input image with a 3 by 3 filter. Here we 
aren’t using any padding, so the filter cannot be centered on the outermost ring of pixels. The outer shapes show the original image and the 
footprint of the 3 by 3 filter as it moves over the image. The central figure 
is the resulting 3 by 3 image.

If we use a single ring of zero-padding the original 5 by 5 image, then 
using the same filter will give us a 5 by 5 image, as in Figure 21.44.
<img src='image/63.png'>

Figure 21.44: The same setup as in Figure 21.43, only now our input 5 by 
5 image has been zero-padded by 1 element. We show several representative placements of the 3 by 3 filter, and the element they produce. The 
output image is 5 by 5, so the input and output have the same sizes.

If we use striding with our filters, then the output will again become 
smaller than the input. For instance, a 60 by 60 padded input with a 
stride of 3 in each direction will produce an output of size 20 by 20. 

Now let’s produce larger inputs than we started with [Dumoulin16]. 
Suppose that we have a starting image of dimensions 3 by 3, and we’d 
like to process it with a 3 by 3 filter. We’d like to end up with a 5 by 5 
image. All we have to do is pad, or surround, the input with two rings 
of 0’s, as in Figure 21.45.

<img src='image/64.png'>

Figure 21.45: Using transposed convolution, our output image can be 
larger than our input. Our original 3 by 3 input is shown in white in the 
outer grids, padded with two elements of 0’s all around. The 3 by 3 filter 
now produces a 5 by 5 result, shown in the center.

We could make the output even larger with more rings of 0’s, but that 
will just produce rings of 0’s in the output. 

Another way to get a larger result is to spread out the input image, by 
inserting padding both around and between the input elements. This 
is called dilated convolution. Let’s insert a single row and column 
of 0’s between each input of or starting 3 by 3 image, and pad all of 
that with 2 rings of 0’s. This turns our 3 by 3 input in a 9 by 9 grid. 
When we sweep our 3 by 3 filter over this grid, we’ll get a 7 by 7 output. 
Thus we’ve enlarged our original 3 by 3 input into a 7 by 7 output, as 
shown Figure 21.46.

<img src='image/65.png'>
Figure 21.46: Dilated convolution. Our original 3 by 3 image is shown in 
the outer grids with white pixels. We’ve inserted a row and column of 0’s 
between each pixel, and then surrounded the whole thing with two rings 
of 0’s. When we convolve our 3 by 3 filter with this grid, we get a 7 by 7 
result, shown in the center.

Let’s make our output even bigger by inserting two rows and columns 
between each original input element, as in Figure 21.47. Now our input 
is 11 by 11, and the output is 9 by 9.
<img src='image/66.png'>

Figure 21.47: The same setup as Figure 21.46, only now we have two rows 
and columns between our original input pixels, producing the 9 by 9 
result in the center.

We can choose as many rows and columns of 0’s as we like between 
our original elements, and as many rings of 0’s as we like around them. 
But we have to keep the size of our filter in mind. If we place, say, 3 
rows and columns of 0’s between each input pixel and we use a filter 
that is 3 elements wide, it will introduce a grid of vertical and horizontal lines of 0’s in the output. We rarely want this, so we usually keep 
the number of extra rows and columns to less than the size of the filter. In this case, using 2 rows and columns is as many as we’d want to apply before convolving with this 3 by 3 filter. This technique of inserting 0’s isn’t foolproof, and can create little checkerboard-like artifacts 
in the output tensors. But these can be avoided by library routines if 
they take steps to handle the convolution and upsampling carefully 
[Odena16]. 

There is a connection between transposed convolution and striding. 
With some imagination, we can describe a transposed convolution 
process like that of Figure 21.47 as using a stride of 1/3 in each dimension. We don’t mean that we literally move 1/3 of a pixel, but rather 
than we need to take 3 steps to move the equivalent of one step in the 
original input. This point of view leads some authors to refer to transposed convolution as fractional striding.

Just as striding lets us combine convolution with a downsampling 
step, transposed convolution (or fractional striding) lets us combine 
convolution with an upsampling step. Both the downsampling and 
upsampling steps can be performed by a layer of the same name. Doing 
the downsampling or upsampling during convolution can produce 
slightly different results than using a separate layer, but experience has 
shown that it’s usually faster and more efficient to do them together, 
and it usually produces results that are just as good [Springenberg15].

# 24.9 An Example Convnet

To demonstrate how we can use convolution layers in practice, let’s 
look at a couple of image classifiers. The first will identify grayscale 
handwritten digits, and the second will identify what object is featured 
in a color photograph, choosing from 1000 different categories. 

Categorizing handwritten digits is a famous problem in machine learning [LeCun89]. We’ll begin by categorizing the handwritten digits in 
the MNIST data set.

The MNIST data set collects tens of thousands of hand-written digits 
from a wide variety of people. The digits are from 0 to 9, and each is 
saved as a grayscale image that’s 28 by 28. Our job is to identify the 
digit in each image. 

We’ll use a simple convnet designed for this job that is included with 
the Keras machine learning library [Chollet17a]. The architecture is 
shown in Figure 21.48 in both our schematic form, and the traditional 
box-and-label form.

<img src='image/67.png'>

The input to the net is the MNIST image, with resolution 28 by 28 by 1. The heart of the convnet is in the first two layers, which perform the 
convolutions. 

The first convolution layer runs 32 filters of size 3 by 3 over the input. 
The result of each filter’s output is run through a ReLU activation function before it leaves the layer.

Keep in mind that we merely tell the system that we want 32 filters 
with a 3 by 3 footprint. It recognizes that the input has just 1 channel, so it creates filters with dimensions 3 by 3 by 1. Since we don’t 
specify an initialization method, the library uses its default (in Keras, 
this is Glorot initialization). The stride is left at the default of 1 in each 
direction, and we apply no padding. As we saw above, this means that 
the outermost ring of pixels won’t make it to the output, so the output 
image will be 26 by 26, but that’s okay because all MNIST images are 
supposed to have a border of 4 black pixels around the digit (not all 
images have this border, but most do).

So the first layer’s input tensor is 28 by 28 by 1, and the output tensor 
is 26 by 26 by 32. 

The second step is another convolution layer, this time with 64 filters 
with a 3 by 3 footprint. The system knows that the input has 32 channels, so each filter is created with size 3 by 3 by 32. The input tensor is 26 by 26 by 32. Because we’re using the default stride and padding 
here as well, we lose another ring around the outside of image, producing an output tensor of shape 24 by 24 by 64. 

We could have used striding to reduce the size of the output, but here 
we use an explicit max pooling layer with blocks of size 2,2. That means 
for every non-overlapping 2 by 2 block in the input, the layer outputs 
just one value containing the largest value in the block. Thus the output of this layer is a tensor of size 12 by 12 by 64 (the pooling doesn’t 
change the number of channels).

Next comes a dropout layer. Since there are no neurons in the max 
pooling layer, this affects the most recent convolution layer, here the 
one with 64 filters. On each epoch, one-quarter of the neurons in this 
layer will be temporarily disabled. This should help hold off overfitting. 

The dropout layer is really just instructions to the code running the 
network, and it doesn’t perform any operations. So the tensor coming 
out of the dropout layer is the same as what went in, or 12 by 12 by 64.

Now we leave the convolutional part of the network, and prepare the 
values for output. These steps, or something like them, are typically 
found at the end of a classification convnet. 

First we flatten the input tensor so it’s a big list of 12×12×64=9216 
numbers. That list goes into a fully-connected, or dense, layer of 128 
neurons. That layer also gets affected by dropout, where a quarter of 
the neurons are temporarily disconnected at the start of each epoch. 

The 128 outputs of this layer go into a final dense layer with 10 neurons. The 10 outputs of this layer go into a softmax step, so that they’re 
converted to probabilities. The 10 numbers that come out of this last 
layer give us the network’s prediction of the probability that the input 
image is the corresponding digit.

We trained the network for 12 epochs using the standard MNIST 
training data. Its accuracy on the training and validation data sets are 
shown in Figure 21.49.

<img src='image/68.png'>

The curves show we’re at about 99% accuracy on both the training and 
validation data sets. Since the curves aren’t diverging, we’ve successfully avoided overfitting. 
Let’s look at some predictions. Figure 21.50 shows some images from 
the MNIST validation set, and the digit corresponding to the network’s 
largest probability. On this little set of examples, it did a perfect job.

This little convnet is doing most of its work in the two convolution layers, and they each just ran little 3 by 3 filters over the image. But that 
was enough to enable the system to correctly identify 99% of the digits 
in the validation set. 
Thanks to how well this technique performs, convolution has become a 
staple technique for deep learning architectures that work with images 
and volumes, as well as the other applications we mentioned at the 
start of this chapter. 
In Chapters 23 and 24 we’ll look at how to actually write code in the 
Keras library to build convnets.

# 24.9.1 VGG16

Let’s now look at a much bigger and more powerful convnet, capable 
of identifying 1000 different objects in color photographs. 

The ILSVRC2014 competition was a public challenge in 2014, asking 
people to build the best neural network they could for classifying a provided database containing a huge variety of images [Russakovsky15]. 
The acronym ILSVRC stands for “Imagenet Large Scale Visual 
Recognition Challenge.” 

The training data contained 1.2 million images, each manually labeled 
with one of the 1000 objects that the network should be able to recognize. The challenge actually included several sub-challenges, each with 
its own winners [Imagenet14]. The winner of one of the classification 
tasks was a network called VGG16 [Simonyan14]. VGG is an acronym 
for the “Visual Geometry Group” who developed the system. The 16 
refers to the network’s 16 computational layers (there are also some 
utility layers for pooling and padding).

The VGG16 system has become popular for working with image 
classifiers. The authors have released all the weights and how they 
pre-processed the training data, and the network itself is easy to understand, modify, and use as a starting point for other networks. 

So we can easily create a full version of VGG16 in our own code and use 
it right away to classify images, with no training time. But if we want 
to experiment with the system, or teach it new tricks, we can start with 
the trained model and change it. 

Let’s look at the VGG architecture. As with the MNIST system, most of 
the work is done by a series of convolution layers. Utility layers appear 
along the way, and then there’s some flattening and dense layers at the 
very end. 

Before each convolution, we pad the image with zeros so that we don’t 
lose pixels around the outside.

The convolution layers (with their zero-padding) come in sequences of 
2 or 3 of the same repeated layer. At the end of each of these sequences 
there’s a max pooling layer with a pooling size of 2 by 2 and a stride of 
2 in each dimension, so the output tensor is cut down by half in both 
width and height. 

Before we feed any data to our model, we must pre-process it in the 
same way that authors pre-processed their training data. That involves 
making sure the image is sized at 224 by 224, and each channel has 
been adjusted by subtracting a specific value from all of its pixels 
[Simonyan14]. Once that’s done, we’re ready to feed our image to the 
network.

We’ll present the VGG16 architecture as a series of 6 blocks. Figure 
21.51 shows the first stage. The input is a color image of 3 channels 
(one each for the red, green, and blue values of each pixel) of size 224 
by 224. That input image goes into two sequential padding-convolution steps, and is then reduced by half in each dimension with a max 
pooling step.

<img src='image/69.png'>
Figure 21.51: The first block of VGG16. The input is zero-padded with a 
single ring of zeros, and then we apply 64 filters each of size 3 by 3. Then 
we zero-pad that result and run another set of 64 filters over it, each 
again with a footprint of 3 by 3. Finally, we use max pooling to reduce the 
size of each of the original image’s dimensions by half.

We can see in Figure 21.51 that we’ve grouped the initial processing 
into two identical chunks of a pooling step followed by convolution. 
The output is a tensor of size 112 by 112 by 64.

This tensor then flows into the next block, shown in Figure 21.52. This 
is just like block one, only we apply 128 filters in each convolution layer.

<img src='image/70.png'>
Figure 21.52: The second block of VGG16 is just like the first block in 
Figure 21.51, except that we use 128 filters in each convolution layer rather 
than 64.

Block 3 continues the pattern of doubling the number of filters in each 
convolution layer. But it also repeats the padding-convolution grouping three times instead of 2. Figure 21.53 shows block 3.

<img src='image/71.png'>
Figure 21.53: Block 3 of VGG16 doubles the number of filters again to 
256, and repeats the padding-convolution step 3 times rather than 2 as 
before.

Blocks 4 and 5 of the network are the same. Each block is built from 
three pairs of padding and convolution, followed by a max pooling 
layer. The structure of these layers is shown in Figure 21.54.

<img src='image/72.png'>

Figure 21.54: Block 4 and Block 5 of VGG16 are the same. They each have 
three pairs of padding and convolution, followed by a 2 by 2 max pooling 
step.

This ends the convolution blocks, and now we come to the wrap-up. 
As with the MNIST network, we first flatten the tensor coming out of 
Block 5. We then run it through two dense layers of 4096 neurons, 
each followed by dropout with an aggressive setting of 50%. Finally, 
the output goes into a dense layer with 1000 neurons, one for each 
category. The results are fed to softmax, which produces our output of 
1000 probabilities, one for each class. Figure 21.55

<img src='image/73.png'>
Figure 21.55: The final steps of processing in VGG16. We flatten the image, 
then run it through two dense layers each using dropout. Then we enter 
a dense layer with 1000 neurons, and pass its output through softmax. 
The result is a list of 1000 probabilities, one for each class the image 
could belong to.

Figure 21.56 shows the whole architecture in one place.
<img src='image/74.png'>

If we were to rebuild VGG16 today, we would probably remove the max 
pooling layers, and instead use 2 by 2 striding in the last convolution 
layer of each repeated chunk. 

In Chapter 20 we saw many examples of using VGG16 on images it 
hadn’t seen before. For fun, Figure 21.57 shows four more pictures 
shot around Seattle on a sunny day. The convnet has never seen these 
images, even during validation, but it does a great job with them.

# 21.9.2 Looking at the Filters, Part 1

VGG16 does a great job at classifying images, thanks in large part to 
the filters that were learned by its convolution layers. It seems like it 
would be instructive to look at the filters and see what they’ve learned. 

But the filters themselves are just 3 by 3, which is too small for us to 
make much sense of them. But we can see them indirectly by looking 
at images that trigger each filter. In other words, once we’ve selected a 
filter we want to visualize, we can find a picture that causes that filter 
to output its biggest value.

We can do this with a little trick that is based on gradient descent, the 
algorithm that we used in Chapter 18 as part of backpropagation. But 
now we’ll use gradient ascent to climb up the gradient. The technique 
starts with an image full of random noise. We measure the output of the 
filter we’re interested in, and then we use the gradients in the network 
to modify the pixels in the input image. We don’t touch the weights or 
anything else in the network itself, since it’s not learning anything. But 
we’re using the gradients to tell us how to modify the pixels so that the 
input image stimulates that filter a little bit more. We do this over and 
over until the filter’s output is as large as we can make it [Zeiler13]. 

In a sense, the resulting image is what the filter is “looking for.” 
Because we start with random values, we’ll get a different final image 
every time, though they’ll all be similar because they’re all based on 
maximizing the same filter. 

Let’s look at some images produced by this method. Figure 21.58 shows 
pictures produced for the 64 filters in the 2nd convolution layer in the 
first block of VGG16 (we’ll use the label block1_conv2 for this layer, 
and similar names for the other layers we’ll look at).

<img src='image/75.png'>

It seems like a lot of these layers are looking for stripes of different 
widths and orientations, which would be a good way to find edges. A 
few seem to be looking for borders of different types, and a bunch have 
values that are too subtle for us to make much of, though the filter in 
the bottom-right looks like a creepy thing with many eyes.

Let’s move up to block 3, and look at the first 64 filters from the first 
convolution layer there. Figure 21.59 shows the images that stimulate 
these filters the most.

<img src='image/76.png'>

Now we’re talking! The filters here seem to be looking for more complex textures, though there are still lots of stripes and stripy patterns. 
Let’s move on even higher and look at the first 64 filters from the first 
convolution layer of block 4, in Figure 21.60.

<img src='image/77.png'>

These are getting interesting. The filters seem to be hunting for patterns that involve a lot of different kinds of flowing and interlocking 
shapes. 
Just for fun, let’s look at close-ups of a few of these filters. Figure 21.61 
shows larger views of 9 patterns from the first few layers.

<img src='image/78.png'>

Figure 21.62 shows patterns that triggered big responses from filters 
in the last few layers.
<img src='image/79.png'>

These patterns are exciting and beautiful. They also have an organic 
feeling about them, probably because VGG16 was trained on a huge 
number of images of animals.

# 21.9.3 Looking at the Filters, Part 2

Another way to look at a filter is to run the same image through VGG16, 
and look at the image produced by that filter. That is, we feed an image 
to VGG16 and let it run through the network, but we ignore the network’s output and instead extract the output produced by the network 
we’re interested in, and we draw that. 
Let’s give it a spin. 
Figure 21.63 shows our input image of a duck. We’ll use this for all of 
our images in this section.

<img src='image/80.png'>

To get a feeling for things, Figure 21.64 shows the response from the 
very first filter on the very first convolution layer of the network. Since 
the output of a filter has just one channel, the image is no longer in 
color. We gave it a heatmap from black to reds to yellow to show the 
value of each element from 0 to 255.

<img src='image/81.png'>

It looks like this filter is trying to find vertical edges. When they get 
darker from top to bottom or left to right, we get a big response. When 
they get lighter in those directions, we get a very small response. 

Figure 21.65 shows the responses from the first 32 filters in the first 
convolution layer of the first block.

<img src='image/82.png'>

A lot of these filters seem to be looking for edges, but others seem to be 
looking for particular features of the image. Let’s look at close-ups of 8 
manually selected filters chosen from all 64 of the filters on this layer, 
shown in Figure 21.66.

<img src='image/83.png'>

The third image in the top row seems to be looking for the duck’s 
feet, or maybe it’s just interested in bright orange things. The leftmost image in the bottom row looks like it’s searching for the waves 
and snow behind the duck, though the image to its right appears to be 
responding most to the blue waves. 

Let’s move farther into the network, out to the third block of convolution layers. The outputs here are smaller by a factor of 4 on each side 
than those coming out of the first block, because they’ve gone through 
two pooling layers of size 2,2. We’d expect that they will be looking 
for clusters of features, and less directly tied to the duck itself. Figure 
21.67 shows the responses for the first convolution layer in block 3.

<img src='image/84.png'>

It’s interesting that there still seems to be a lot of edge finding going on. 
It seems that strong edges are an important cue for VGG16 as it works 
to figure out what an image is showing. But there are lots of regions 
that are bright, perhaps where the texture of the image matches one or 
more of the patterns that the filters are looking for. 
Let’s jump all the way to the last block. Figure 21.68 shows the 
responses for the first 32 filters for the first convolution layer in block 5

<img src='image/85.png'>
As we’d expect, these images are even smaller, having passed through 
two more pooling layers that each reduce the size by a factor of 2 on 
each side. At this point the duck is hardly visible, as the system is combining features from the previous layers. Some of the filters are barely 
responding. They are probably responsible for finding high-level features that aren’t present in the duck image. 

In Chapter 28 we’ll look at a couple of creative applications that use 
the filter responses in a convnet to create art.

# 21.10 Adversaries

There’s a surprising thing that we can do to our images that will throw 
off VGG16’s predictions. In fact, this trick will mess up the results of 
any classifier. 

The trick to “fooling” our convnet involves creating a new image called 
an adversary. This image is created from the starting image by 
adding an adversarial perturbation (or more simply, a perturbation), which is an image that looks like random noise to us. If the same perturbation works for every image we give to a particular classifier, or 
even to every image we give to every classifier, we call it a universal 
perturbation [Moosavi-Dezfooli16]. 

Suppose that when we get ready to hand a picture to our classifier, we 
first add this perturbation to it, pixel by pixel. The changes are so small 
that we can’t see any difference, even when looking at the before and 
after pictures side by side. But the little changes to the pixels are just 
right to cause the classifier to completely mess up and predict what 
appears to be a random category. 

For example, on the left of Figure 21.69 we see an image of a tiger. The 
system correctly classifies it as a tiger with about 80% confidence, with 
a little bit of confidence for related animals such as a tiger cat (a small 
forest cat) and a jaguar

<img src='image/86.png'>

Figure 21.69: An adversarial attack on an image. Left: At top, a picture 
of a tiger. Below it are the classes predicted by VGG16, with their probabilities. Middle: An image created by a program that wants to cause the 
tiger to be misclassified. We’ve expanded the pixel values so they’re visually readable in the figure, but the values at the top show that the values 
are in the range of about −2 to +2. Right: At top, the result of adding 
the middle image to the tiger in the upper left. Visually, the tiger looks 
unchanged. Below, the results of VGG16 on the image above. It’s not even 
recognized as an animal!

In the middle of Figure 21.69 we show an image computed by an algorithm designed to find adversaries. We’ve cranked up the values so we 
can see them better, but the numbers at the top show that the pixels 
are all in the range of about −2 to +2 (the tiger’s pixels are all in the 
range 0 to 255). When we add this seemingly noisy image to the tiger, 
we get the image on the right. To our eyes, it seems unchanged. But 
look at the change in the classes! The system doesn’t even think that 
this is an animal.

A variety of algorithms have been developed for creating adversarial 
images [Rauber17a]. The range of values in the perturbations these 
methods create for a given image can vary considerably, so to find the 
smallest perturbation it’s often worth trying a few different methods. 
We can tell these methods, called attacks, what criteria they should 
use in order to measure success. For example, we can ask for a perturbation that simply causes the input to be misclassified. Another option 
asks for a perturbation that will cause the input to be classified as a 
particular class. For the images above, we asked for a perturbation 
such that none of the original top 7 classes assigned by the classifier 
would show up as the top class for the adversary. Many of these methods have been implemented in a Python library that we can use with 
any type of classifier and any image [Rauber17b].

Adversarial perturbations have to be carefully constructed. And they 
may be an inevitable weakness of CNNs [Gilmer18]. But their existence 
suggests that convnets still hold surprises for us, and they shouldn’t be 
considered foolproof. There’s more to be learned about what’s going 
on inside of CNNs.


```python

```
